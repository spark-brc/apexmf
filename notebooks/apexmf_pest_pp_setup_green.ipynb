{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upper Green PEST Pilot Point Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, shutil\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import flopy as flopy\n",
    "import pyemu\n",
    "import shapefile #the pyshp module\n",
    "from pyemu.pst.pst_utils import SFMT,IFMT,FFMT,pst_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:yellow\">1. Set up pilot points network for Green model some</span>.\r\n",
    "\r\n",
    "There are multiple approaches to implementing pilot points with PEST++.  \r\n",
    "\r\n",
    "In this class, we will use some kick-ass pyemu sweetness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Set up zones for where pilot points will be interpolated\n",
    "\n",
    "We can have pilot point networks in multiple zones. In this case, we will make a simple zone file using `IBOUND` such that all active cells are in the same interpolation zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = 'D:/Projects/Watersheds/Green/Analysis/APEX-MODFLOWs/gr_012721/APEX-MODFLOW/MODFLOW'\n",
    "mname = \"gr_1000.nam\"\n",
    "os.chdir(working_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = flopy.modflow.Modflow.load(fs.MODEL_NAM,model_ws=wd,load_only=[]) #<-- load only prevents reading ibound\n",
    "m = flopy.modflow.Modflow.load(\n",
    "            mname,\n",
    "            model_ws=working_dir\n",
    "            )\n",
    "m.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.bas6.ibound[0].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. It is for when pilot points don't exist. We don't want pilot points or care about HK values in inactive cells, but we do need values in constant heads\n",
    "\n",
    "We are going to use a pyemu helper function to setup pilot points are cell centers for active cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pilot points as a shapefile\n",
    "# we want hk pilot points in the top layer...\n",
    "prefix_dict = {0:[\"sy0\"]}\n",
    "df_pp_hk = pyemu.pp_utils.setup_pilotpoints_grid(ml=m,\n",
    "                                              prefix_dict=prefix_dict,\n",
    "                                              pp_dir=working_dir,\n",
    "                                              tpl_dir=working_dir,\n",
    "                                              every_n_cell=10,\n",
    "                                              shapename='pp_sy.shp')\n",
    "# pp_file = os.path.join(working_dir,\"sypp.dat\")\n",
    "# assert os.path.exists(pp_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Create dataframe from shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.3.1. Change Shapefile name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change shapefile and file name\n",
    "shpwd = working_dir\n",
    "shp = 'pp_sy.shp'\n",
    "shp_changed = 'sy0pp.shp'\n",
    "ppf = shp_changed[:-3] + 'dat'\n",
    "ppf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.3.2. Shapefile to Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read file, parse out the records and shapes\n",
    "shapefile_path = os.path.join(shpwd, shp)\n",
    "sf = shapefile.Reader(shapefile_path)\n",
    "\n",
    "#grab the shapefile's field names (omit the first psuedo field)\n",
    "fields = [x[0] for x in sf.fields][1:]\n",
    "records = sf.records()\n",
    "shps = [s.points for s in sf.shapes()]\n",
    "\n",
    "#write the records into a dataframe\n",
    "shapefile_dataframe = pd.DataFrame(columns=fields, data=records)\n",
    "\n",
    "#add the coordinate data to a column called \"coords\"\n",
    "shapefile_dataframe = shapefile_dataframe.assign(coords=shps)\n",
    "\n",
    "pp_df = shapefile_dataframe.sort_values(by=['name'])\n",
    "print(pp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.utils.pp_utils.write_pp_file(ppf, pp_df)\n",
    "# pyemu.utils.pp_utils.pilot_points_to_tpl('hahaha.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PP_FMT = {\"name\": SFMT, \"x\": FFMT, \"y\": FFMT, \"zone\": IFMT, \"tpl\": SFMT,\n",
    "          \"parval1\": FFMT}\n",
    "def pp_to_tpl(pp_file, tpl_file=None):\n",
    "    names = pp_df['parnme'].tolist() # for hk\n",
    "    # names = pp_df['sypar'].tolist() # for sy  \n",
    "#     names = pp_df['parnme'].tolist() # for river conductance      \n",
    "    if tpl_file is None:\n",
    "        tpl_file = pp_file + \".tpl\"    \n",
    "    tpl_entries = [\"~    {0}    ~\".format(name) for name in names]\n",
    "    pp_df.loc[:,\"tpl\"] = tpl_entries\n",
    "    pp_df.loc[:,\"parnme\"] = names\n",
    "\n",
    "\n",
    "    f_tpl = open(tpl_file,'w')\n",
    "    f_tpl.write(\"ptf ~\\n\")\n",
    "    f_tpl.write(pp_df.to_string(col_space=0,\n",
    "                              columns=[\"name\",\"x\",\"y\",\"zone\",\"tpl\"],\n",
    "                              formatters=PP_FMT,\n",
    "                              justify=\"left\",\n",
    "                              header=False,\n",
    "                              index=False) + '\\n')    \n",
    "pp_to_tpl(ppf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at ``pp_df`` - it has a lot of useful info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So cool, we now defined pilot points as a set of spatially distributed parameters...but how do go from pilot points to the model input HK array? Answer: geostatistics.  We need to calculate the geostatistical factors (weights) used to form the interpolated value for the HK value at each model cell - its a spatially-weighted combination of pilot point values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">2. Geostatistics</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to create Kriging factors and regularization inputs\n",
    "Following the guidelines in _Approaches to Highly Parameterized Inversion: Pilot-Point Theory, Guidelines, and Research Directions_ https://pubs.usgs.gov/sir/2010/5168/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we need to define a couple geostatistical structures (e.g. variograms)\n",
    "\n",
    "From _PEST Groundwater Data Utilities Part A: Overview_ page 43, there are 4 acceptable variogram types:\n",
    "\n",
    " 1. *Spherical*  \n",
    "### $\\gamma\\left(h\\right)=c\\times\\left[1.5\\frac{h}{a}-0.5\\frac{h}{a}^3\\right]$ if $h<a$\n",
    "### $\\gamma\\left(h\\right)=c$ if $h \\ge a$  \n",
    "     \n",
    " 2. *Exponential*  \n",
    "### $\\gamma\\left(h\\right)=c\\times\\left[1-\\exp\\left(-\\frac{h}{a}\\right)\\right]$  \n",
    "     \n",
    " 3. *Gaussian*  \n",
    "### $\\gamma\\left(h\\right)=c\\times\\left[1-\\exp\\left(-\\frac{h^2}{a^2}\\right)\\right]$  \n",
    " \n",
    " 4. *Power*  \n",
    "### $\\gamma\\left(h\\right)=c\\times h^a$\n",
    "     \n",
    " The number refers to `VARTYPE`. `BEARING` and `ANISOTROPY` only apply if there is a principal direction of anisotropy. $h$ is the separation distance, and $a$ is the range, expressed with the `A` parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's create ``variogram`` and ``GeoStruct`` objects.  \n",
    "\n",
    "These describe how HK varies spatailly, remember?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pyemu.geostats.ExpVario(contribution=0.8,a=30000, bearing=0)\n",
    "gs = pyemu.geostats.GeoStruct(variograms=v,nugget=0.0)\n",
    "ax = gs.plot()\n",
    "ax.grid()\n",
    "# ax.set_ylim(0,2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get an ``OrdinaryKrige`` object, which needs the ``GeoStruct`` as well as the x, y, and name of the pilot point locations (which happens to be in that really cool ``df_pp`` instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = pyemu.geostats.OrdinaryKrige(gs,pp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the ``OrdinaryKrige`` is created, we need to calculate the geostatistical interpolation factors for each model cell.  We do this with the ``.calc_factors_grid()`` method: it needs to know about the model's spatial orientation and also accepts some optional arguments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kriging Processing... it takes time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ok.calc_factors_grid(m.sr,\n",
    "#                           var_filename=pst_name.replace(\".pst\",\".var.ref\"),\n",
    "                          var_filename= ppf[:-3] + \"var.ref\",                          \n",
    "                          minpts_interp=1,maxpts_interp=50,\n",
    "                          search_radius=1000000000000.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the really cool things about geostatistics is that it gives you both the interpolation (factors), but also gives you the uncertainty in the areas between control (pilot) points.  Above, we wrote this uncertainty information to an array that has the same rows and cols as the model grid - this array is very useful for understanding the function of the variogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# arr_var = np.loadtxt(pst_name.replace(\".pst\",\".var.ref\"))\n",
    "arr_var = np.loadtxt(ppf[:-3] + \"var.ref\")\n",
    "ax = plt.subplot(111,aspect=\"equal\")\n",
    "p = ax.imshow(arr_var,extent=m.sr.get_extent(),alpha=0.25)\n",
    "plt.colorbar(p)\n",
    "plt.tight_layout()\n",
    "ax.scatter(pp_df.x,pp_df.y,marker='.',s=4,color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that at the pilot point locations (red dots), the uncertainty in the geostats is minimal...as expected. The call to ``.calc_factors_grid()`` also returns a ``DataFrame`` which has useful info - lets look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is one row for each model cell, and for each row, we see the distance, names, and weight for the \"nearby\" pilot points.  The interpolated value for cells that have a pilot point at their center only need one weight - 1.0 - and one pilot point.  Other cells are weighted combinations of pilot points.  Is this clear?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to save the factors (weights) to a special file that we will use later to quickly generate a new HK array from a set of pilot point values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok.to_grid_factors_file(ppf+\".fac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for demo purposes, lets generate ``random`` pilot point values and run them through the factors to see what the ``hk`` array looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random values\n",
    "pp_df.loc[:,\"parval1\"] = np.random.random(pp_df.shape[0])\n",
    "# save a pilot points file\n",
    "pyemu.pp_utils.write_pp_file(ppf,pp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate the pilot point values to the grid\n",
    "hk_arr = pyemu.utils.geostats.fac2real(ppf,factors_file=ppf+\".fac\",out_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "ax = plt.subplot(111,aspect='equal')\n",
    "ax.imshow(hk_arr,interpolation=\"nearest\",extent=m.sr.get_extent(),alpha=0.5)\n",
    "ax.scatter(pp_df.x,pp_df.y,marker='.',s=4,color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if you recalculate the factors using one point for every cell? Change ``max_interp_pts`` to 1 in the ``calc_factors_grid()`` and rerun these cells..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An aside on geostatistics and covariance matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``GeoStruct`` object above was used to interpolate from pilot point locations to each node in the grid.  But this same ``GoeStruct`` also has an important information regarding how the pilot points are related to each other spatially---that is, the ``GeoStruct`` object implies a covariance matrix.  Let's form that matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = gs.covariance_matrix(pp_df.x,pp_df.y,pp_df.parnme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cov.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these numbers mean?  Why should you care?  Well, this covariance matrix plays an important role in uncertainty quantification, as well as in governing the way pilot point parameters are adjusted during calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build instruction files (Streamflow / Watertable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Streamflow (output.rch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, 'D:/spark-brc_gits/apexmf_git/apexmf_pkgs')\n",
    "# from apexmf_pst_pkgs import apexmf_pst_utils, apexmf_pst_par\n",
    "import apexmf_pst_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"D:/Projects/Watersheds/Green/Analysis/APEX-MODFLOWs/calibrations/gr_210614/APEX-MODFLOW\"\n",
    "os.chdir(working_dir)\n",
    "wd = \"D:/Projects/Watersheds/Green/Analysis/APEX-MODFLOWs/calibrations/gr_210614/APEX-MODFLOW/MODFLOW\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create parm template file\n",
    "\n",
    "sw_par = apexmf_pst_utils.parm_to_tpl_file()\n",
    "sw_par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Create river parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide channel ids that will be used for calibration\n",
    "subs = ['rg009', 'rg096', 'rg199', 'rg155', 'rg200']\n",
    "apexmf_pst_par.create_riv_par(wd, subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a template file for mf_riv.par file\n",
    "apexmf_pst_utils.riv_par_to_template_file('mf_riv.par')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite the river package file\n",
    "apexmf_pst_par.riv_par(wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2. Build instruction files (streamflow / watertable / baseflow)\n",
    "## 1.2.1. Streamflow (output.rch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path\n",
    "rch_file = 'SITE199.RCH'\n",
    "# reach numbers that are used for calibration\n",
    "subs = [9, 96, 199]\n",
    "# extract month_streamflow\n",
    "apexmf_pst_utils.extract_month_str(rch_file, subs, '1/1/1990', '1/1/2000', '12/31/2012')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3. Create instruction files for each str_sim file using the 'streamflow.obd' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because we have 3 streamgages let's loop for them\n",
    "# read streamobd and get column names\n",
    "stf_obd = pd.read_csv(\n",
    "                    'streamflow_month.obd',\n",
    "                    sep='\\t',\n",
    "                    index_col=0,\n",
    "                    parse_dates=True,\n",
    "                    na_values=[-999, '']\n",
    "                    )\n",
    "# stf_obd_c = stf_obd.resample('M').mean()\n",
    "# stf_obd_c.to_csv('streamflow_m.obd', sep='\\t', na_rep=-999, float_format='%.2f')\n",
    "obds = stf_obd.columns.tolist()[::-1]\n",
    "# obds.remove('sub046')\n",
    "# obds.remove('sub130')\n",
    "print(obds)\n",
    "sim_files = ['cha_{:03d}.txt'.format(x) for x in subs]\n",
    "# sed_files = ['sed_{:03d}.txt'.format(x) for x in subs]\n",
    "# sim_files = sim_files + sed_files\n",
    "print(sim_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instruction files for each sim file\n",
    "for i in range(len(sim_files)):\n",
    "    apexmf_pst_utils.stf_obd_to_ins(sim_files[i], obds[i], '1/1/2000', '12/31/2012', time_step='month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get groundwater levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtdf = pd.read_excel(\"D:/Projects/Watersheds/Green/GIS/mf_obs_grids.dbf.xlsx\", engine=\"openpyxl\")\n",
    "# get cal only\n",
    "wtdf = wtdf.loc[wtdf.cal_counts > 0]\n",
    "wtdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(wd)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do have watertable data now\n",
    "grid_ids = wtdf.grid_id.to_list()\n",
    "\n",
    "apexmf_pst_utils.extract_watertable_sim(grid_ids, '1/1/1995', '12/31/2012')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grid_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obd_names = wtdf.name.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'wt{:05d}'.format(3087)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grid_id, obd_name in zip(grid_ids, obd_names):\n",
    "    apexmf_pst_utils.mf_obd_to_ins('wt_{}.txt'.format(grid_id), 'wt{:05d}'.format(grid_id), '1/1/1995', '12/31/2012')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "io_files = pyemu.helpers.parse_dir_for_io_files('.')\n",
    "pst = pyemu.Pst.from_io_files(*io_files)\n",
    "pyemu.helpers.pst_from_io_files(io_files[0], io_files[1], io_files[2], io_files[3], 'green_dummy.pst')\n",
    "\n",
    "# print(os.chdir(\"..\"))\n",
    "io_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``parse_dir_for_io_files()`` helper is looking for files with the \".tpl\" and \".ins\" extension.  This assumes that the corresponding model input and model output files are the same name, minus the \".tpl\" and \".ins\" extension, respectively.  These file lists are then passed to another helper, which builds a basic control file for you (``Pst.from_io_files()``).  Let's look at this generic ``Pst`` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = pst.parameter_data\n",
    "par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Change parameter group name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(par)):\n",
    "    if (par.iloc[i, 0][:2]) == 'sy':\n",
    "        par.iloc[i, 6] = 'sy'\n",
    "    elif par.iloc[i, 0][:7] == 'rivbot_':\n",
    "        par.iloc[i, 6] = 'rivbot'\n",
    "    elif par.iloc[i, 0][:6] == 'rivcd_':\n",
    "        par.iloc[i, 6] = 'rivcd'\n",
    "    elif par.iloc[i, 0][:2] == 'hk':\n",
    "        par.iloc[i, 6] = 'hk'\n",
    "    elif par.iloc[i, 0][:1] == 'p':\n",
    "        par.iloc[i, 6] = 'apex'\n",
    "    # else:\n",
    "    #     par.iloc[i, 6] = 'str'\n",
    "par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = par.sort_values(by=['pargp', 'parnme'])\n",
    "par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Set par ranges and values for MODFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for MODFLOW parameters\n",
    "for i in range(len(par)):\n",
    "    if (par.iloc[i, 6]) == 'hk':\n",
    "        par.iloc[i, 4] = 1.100000e-03\n",
    "        par.iloc[i, 5] = 1.100000e+03\n",
    "    elif par.iloc[i, 6] == 'sy':\n",
    "        par.iloc[i, 3] = 0.100000e+00 \n",
    "        par.iloc[i, 4] = 1.000000e-03\n",
    "        par.iloc[i, 5] = 0.800000e+00        \n",
    "    elif par.iloc[i, 6] == 'rivcd':\n",
    "        par.iloc[i, 3] = 50.01   # initial    \n",
    "        par.iloc[i, 4] = 0.1   # lower\n",
    "        par.iloc[i, 5] = 100   # upper\n",
    "        par.iloc[i, 8] = -50   # offset\n",
    "    elif par.iloc[i, 6] == 'rivbot':\n",
    "        par.iloc[i, 3] = 5.0001   # initial    \n",
    "        par.iloc[i, 4] = 0.1   # lower\n",
    "        par.iloc[i, 5] = 10   # upper\n",
    "        par.iloc[i, 8] = -5   # offset\n",
    "#     else:\n",
    "#         par.iloc[i, 6] = 'str_par'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.parameter_data = apexmf_pst_utils.export_pardb_pest(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = pst.parameter_data\n",
    "par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Cool - the other tpl files were found and parsed - parameter listed in them were added to the control file.  But we have generic entries for initial values bounds..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obd = pst.observation_data\n",
    "print(obd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(obd)):\n",
    "    if obd.iloc[i, 0][:6] == 'sub009':\n",
    "        obd.iloc[i, 3] = 'sub009'\n",
    "    elif obd.iloc[i, 0][:6] == 'sub096':\n",
    "        obd.iloc[i, 3] = 'sub096'\n",
    "    elif obd.iloc[i, 0][:6] == 'sub199':\n",
    "        obd.iloc[i, 3] = 'sub199'\n",
    "    else:\n",
    "        obd.iloc[i, 3] = obd.iloc[i, 0][:7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Import measured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwt_obd = pd.read_csv('MODFLOW/modflow.obd',\r\n",
    "                       sep='\\t',\r\n",
    "                       index_col = 0,\r\n",
    "                       parse_dates = True,\r\n",
    "                       na_values=[-999, '']\r\n",
    "                     )\r\n",
    "gwt_obd = gwt_obd['1/1/2000': '12/31/2012']\r\n",
    "gwt_obd = gwt_obd.dropna(axis=1, how='all')\r\n",
    "\r\n",
    "gwtcolnams = gwt_obd.columns.tolist()\r\n",
    "# gwtcols = [i if len(i) < 7 for i in gwtcolnams]\r\n",
    "gwtcols = [i for i in gwtcolnams if len(i) <= 7]\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwt_obd = gwt_obd[gwtcols]\n",
    "gwt_obd = gwt_obd.reindex(sorted(gwt_obd.columns), axis=1)\n",
    "gwt_obd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwtcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stf_obd = pd.read_csv('streamflow_month.obd',\n",
    "                       sep='\\t',\n",
    "                       index_col = 0,\n",
    "                       parse_dates = True,\n",
    "                       na_values=[-999, '']\n",
    "                     )\n",
    "stf_obd = stf_obd['1/1/2000': '12/31/2012']\n",
    "# stf_obd = stf_obd.drop(['sub046', 'sub130'], axis=1)\n",
    "stf_obd =  stf_obd.reindex(sorted(stf_obd.columns), axis=1)\n",
    "stf_obd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sub list based on obd order\n",
    "sub_order = []\n",
    "for i in obd.obgnme.tolist():\n",
    "    if i not in sub_order:\n",
    "        sub_order.append(i)\n",
    "sub_order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total list from each sub obd, delete na vals\n",
    "tot_obd = []\n",
    "for i in sub_order[:3]:\n",
    "    tot_obd += stf_obd[i].dropna().tolist()\n",
    "for j in sub_order[3:]:\n",
    "    tot_obd += gwt_obd[j].dropna().tolist()    \n",
    "len(tot_obd)\n",
    "# tot_obd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obd.loc[:, 'obsval'] = tot_obd\n",
    "obd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Export control file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.control_data.noptmax=0\n",
    "pst.model_command = 'python forward_run.py'\n",
    "pst.write('green_pest.pst')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also cool - the instruction files in the directory were also found and parsed so that observation listed in the instruction files were added as well. There are some subtlies here, but we will skip them for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in pyemu, we can add two forms of regularization:\n",
    "- preferred value: we want the parameter values to stay as close to the initial values as possible\n",
    "- preferred difference: we prefer the differences in parameter values to be minimized\n",
    "\n",
    "Preferred value is easy to understand, we simply add ``prior_information`` to the control file to enforce this condition.  pyemu uses a helper for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-constructed pst\n",
    "pst = pyemu.Pst(os.path.join(working_dir,pst_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.helpers.zero_order_tikhonov(pst,parbounds=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.prior_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's fine, but should the weight on preferring HK not to change be the same as preferring recharge not to change?  Seems like we would want recharge to change less than HK.  This preference can be expressed by using the parameter bounds to form the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.helpers.zero_order_tikhonov(pst,parbounds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.prior_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are really preferring recharge not to change...good!\n",
    "\n",
    "So what about preferred difference regularization?  Well pyemu can do that too.  Remember that ``Cov``ariance matrix we built above? It expresses the spatial relationship between pilot points, so we use to setup these prior information equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.helpers.first_order_pearson_tikhonov(pst,cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.prior_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened?  We replace the preferred value equations with a bunch of new equations.  These equations each include two parameter names and have different weights - can you guess what the weights are?  The weights are the pearson correlation coefficients (CC) between the pilot points (remember those from way back?).  These CC values are calculated from the covariance matrix, which is implied by the geostatistical structure...whew! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For river Bed conductance interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate the pilot point values to the grid\n",
    "riv_cond = pyemu.gw_utils.fac2real(ppf,factors_file=ppf+\".fac\",out_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(riv_cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. get only river grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_riv = pd.read_csv(\n",
    "                    shpwd + \"\\\\mf\\\\ss_072519.riv\",\n",
    "                    delim_whitespace=True,\n",
    "                    skiprows=3,\n",
    "#                     usecols=[1,2],\n",
    "                    header=None\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_riv_cf = [riv_cond[df_riv.iloc[i, 0], df_riv.iloc[i, 1]] for i in range(len(df_riv))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_riv.iloc[:, 4] = new_riv_cf\n",
    "df_riv.iloc[:, 4] = df_riv.iloc[:, 4].map(lambda x: '{:.10e}'.format(x))\n",
    "df_riv.iloc[:, 3] = df_riv.iloc[:, 3].map(lambda x: '{:.10e}'.format(x))\n",
    "df_riv.iloc[:, 5] = df_riv.iloc[:, 5].map(lambda x: '{:.10e}'.format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(shpwd + \"\\\\mf\", \"ss_072519.riv\"), 'w') as f:\n",
    "    f.write(\"# RIV: River package file created on 7/25/2019 by ModelMuse version 4.0.0.0.\" + \"\\n\")\n",
    "    f.write(\"  1467     9 AUXILIARY IFACE # DataSet 2: MXACTC IRIVCB Option\" + \"\\n\")\n",
    "    f.write(\"  1467     0 # Data Set 5: ITMP NP Stress period 1\" + \"\\n\")    \n",
    "    df_riv.to_csv(f, sep='\\t',\n",
    "                  header=False,\n",
    "                  index=False,\n",
    "#                   float_format='%.2f', \n",
    "                  line_terminator='\\n', \n",
    "                  encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build instruction files (Streamflow / Watertable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Streamflow (channel_day.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "wd2 = \"D:\\\\Projects\\\\MiddleBosque\\\\Analysis\\\\SWAT+MODFLOW Model_middle_bosque\"\n",
    "df_str = pd.read_csv(\n",
    "                    wd2 + \"\\\\channel_day - Copy.txt\",\n",
    "                    delim_whitespace=True,\n",
    "                    skiprows=3,\n",
    "#                     usecols=[1,2],\n",
    "                    header=None\n",
    "                    )\n",
    "test = []\n",
    "for i in range(len(df_str)):\n",
    "    if df_str.iloc[i, 6] == 'cha53':\n",
    "        a = 'l1 w w w w w w w w w '\n",
    "        b = '!str_{}{:02d}{:02d}!'.format(df_str.iloc[i, 3], df_str.iloc[i, 1], df_str.iloc[i, 2])\n",
    "        test.append(a+b)\n",
    "    else:\n",
    "        a = 'l1'\n",
    "        test.append(a)\n",
    "\n",
    "\n",
    "with open('str.ins', \"w\", newline='') as f:\n",
    "    f.write(\"pif ~\" + \"\\n\")  \n",
    "    writer = csv.writer(f)\n",
    "    for row in test:\n",
    "        writer.writerow([row])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Watertable (modflow_cell_obs.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "st_date = '1/1/1980'\n",
    "wd2 = \"D:\\\\Projects\\\\MiddleBosque\\\\Analysis\\\\SWAT+MODFLOW Model_middle_bosque\"\n",
    "df_wt = pd.read_csv(\n",
    "                    wd2 + \"\\\\modflow_cell_obs.txt\",\n",
    "                    delim_whitespace=True,\n",
    "                    skiprows=1,\n",
    "#                     usecols=[1,2],\n",
    "                    header=None\n",
    "                    )\n",
    "df_wt.index = pd.date_range(st_date, periods=len(df_wt))\n",
    "# print(df_str)\n",
    "\n",
    "co1L = []\n",
    "co2D = []\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(len(df_str)):\n",
    "#     if ((df_wt.index[i].strftime('%Y%m%d') >= '19850821') & (df_wt.index[i].strftime('%Y%m%d') <= '19860507')):\n",
    "#         print('true')\n",
    "df_wt['date'] = df_wt.index.strftime('%Y%m%d')\n",
    "# df_wt['test'] = CO1L['8/21/1985':'5/7/1986']\n",
    "# df_wt['test'] = df_wt[(df_wt.index >= '08/21/1985') & (df_wt.index <= '05/07/1986')] = 'w'\n",
    "df_wt['2nd'] = np.where((df_wt.index >= '08/21/1985') & (df_wt.index <= '05/07/1986'), ('l1 !wt_2nd'+df_wt['date']+'!'), 'l1 w')\n",
    "df_wt['1st'] = np.where((df_wt.index >= '09/30/1985') & (df_wt.index <= '04/01/1986'), ('!wt_1nd'+df_wt['date']+'!'), ' w')\n",
    "\n",
    "\n",
    "print(df_wt)\n",
    "\n",
    "with open('wt.ins', \"w\", newline='') as f:\n",
    "    f.write(\"pif ~\" + \"\\n\")\n",
    "    df_wt.to_csv(f, sep='\\t',\n",
    "                  header=False,\n",
    "                  index=False,\n",
    "#                   float_format='%.2f', \n",
    "                  line_terminator='\\n', \n",
    "                  columns=('2nd','1st'),\n",
    "                  encoding='utf-8')\n",
    "\n",
    "'''\n",
    "test = []\n",
    "for i in range(len(df_str)):\n",
    "    if df_str.iloc[i, 6] == 'cha53':\n",
    "        a = 'l1 w w w w w w w w '\n",
    "        b = '!str_{}{:02d}{:02d}!'.format(df_str.iloc[i, 3], df_str.iloc[i, 1], df_str.iloc[i, 2])\n",
    "        test.append(a+b)\n",
    "    else:\n",
    "        a = 'l1'\n",
    "        test.append(a)\n",
    "\n",
    "\n",
    "with open('str.ins', \"w\", newline='') as f:\n",
    "    f.write(\"pif ~\" + \"\\n\")  \n",
    "    writer = csv.writer(f)\n",
    "    for row in test:\n",
    "        writer.writerow([row])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.full((33, 55), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('test.txtt', a, fmt='%.12e', delimiter='\\t')\n",
    "np.savetxt('vtest.txtt', a/10, fmt='%.12e', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.loadtxt('test.txtt')\n",
    "b/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hk04.dat', 'r') as f:\n",
    "    data = [x.strip().split() for x in f if x.strip()]\n",
    "hk = float(data[0][4])\n",
    "hk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fac = ['hk01.dat', 'hk02.dat', 'hk03.dat', 'sy01.dat', 'sy02.dat', 'sy03.dat']\n",
    "for i in data_fac:\n",
    "    if i[:2] == 'hk':\n",
    "        print('true')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "e2cd4a2c4044a442375d541a7af880af35b9f88daa7fc56a2d2af487495916f8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('sm_pest': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}